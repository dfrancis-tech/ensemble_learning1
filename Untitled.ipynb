{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83649c15-f3db-4d93-b7db-bb3c9808b411",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "* Versatile: Suitable for both classification and regression, making it flexible for various problem types.\n",
    "* Minimal Data Preprocessing: Works well with different data types and handles missing values, requiring minimal data handling before use.\n",
    "* Feature Importance: Provides insight into feature importance, allowing a better understanding of each feature’s impact on the model.\n",
    "* Fast Predictions: While training is sequential, predictions are made in parallel, which makes boosting efficient at prediction time.\n",
    "* Black Box Model: Boosting lacks transparency; it’s challenging to understand how predictions are made due to the complexity of many underlying models.\n",
    "* Training Time: Sequential training process is slow and can be computationally demanding.\n",
    "* Overfitting Risk: Has a tendency to overfit, especially with noisy data, due to its iterative learning process that tries to correct errors.\n",
    "- Boosting is powerful and should be considered for many machine learning tasks, but it's essential to watch for long training times and overfitting risks.\n",
    "- **Types of Boosting**\n",
    "  \n",
    "- Bias Reduction:\n",
    "Adaptive Boosting: Reduces bias by oversampling misclassified examples from previous weak models.\n",
    "Gradient Boosting: Reduces bias by training on actual errors (residuals) instead of oversampling misclassified examples.\n",
    "- Base Models:\n",
    "Adaptive Boosting: Can accept various algorithms as base models but typically uses decision trees.\n",
    "Gradient Boosting: Always uses decision trees as its base model.\n",
    "- Optimization Method:\n",
    "Adaptive Boosting: Optimizes the strong learner using weighted voting based on the performance of each weak model.\n",
    "Gradient Boosting: Utilizes gradient descent optimization for its learning process.\n",
    "- Loss Functions:\n",
    "Adaptive Boosting: Employs an exponential loss function.\n",
    "Gradient Boosting: More flexible with loss functions, accepting both exponential and deviance.\n",
    "- XGBoost:\n",
    "Stands for Extreme Gradient Boosting, an optimized implementation of gradient boosting focused on speed.\n",
    "Shares many details with gradient boosting but is designed for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9532c9-1db6-4345-a727-3999c76c9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38232215-e8a7-4345-952b-7edc9668ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the hyper parameters for Gradient Boosting Classifier\n",
    "GradientBoostingClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c5e358-92f3-489d-b669-5f1a65322070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the hyper parameters for AdaBoost Classifier\n",
    "AdaBoostClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c68e8c-24b5-4cb1-b814-d608c90fcb95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
