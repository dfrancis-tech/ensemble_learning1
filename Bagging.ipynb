{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f215d74a-0e9e-4118-aa51-141d05065cb3",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "Bagging (short for bootstrap aggregating) is an ensemble method that combines multiple independent weak models, often deep decision trees, to create a stronger overall model.It falls under ensemble learning, similar to boosting, and is based on aggregating predictions from multiple models trained in parallel.\n",
    "- **Comparison with Boosting**:\n",
    "Bagging uses low-bias, high-variance models (e.g., deep trees). Boosting uses high-bias, low-variance models (e.g., shallow trees).\n",
    "In bagging, models are trained independently and in parallel, making it faster.\n",
    "In boosting, models are trained sequentially, where each model corrects errors from previous models, making it slower.\n",
    "- **Bootstrap Aggregation**:\n",
    "Bootstrap means sampling with replacement, so data points can appear in multiple samples.\n",
    "Each sample of training data generates a deep decision tree independently, targeting different relationships in the data.\n",
    "- **Prediction Process**:\n",
    "During testing, each tree makes a prediction based on the test example.\n",
    "The model aggregates predictions (e.g., via voting) from all trees to make the final prediction, improving accuracy through diversity in the independent predictions.\n",
    "- **Advantages**:\n",
    "Parallel Training: Faster due to independent training of each tree.\n",
    "Effective for High Variance: Reduces variance by combining predictions from deep, high-variance trees.\n",
    "Good for Uncorrelated Trees: Each treeâ€™s independence helps capture unique data relationships, leading to better final predictions.\n",
    "In short, bagging offers a reliable method for improving accuracy and robustness in models by aggregating predictions from independent, deep decision trees.\n",
    "- **When to Use Bagging**:\n",
    "Suitable for problems with categorical or continuous target variables.\n",
    "Provides feature importance, helping to understand feature relationships with the target variable.\n",
    "Flexible and fast to train; performs well on most problems, making it a good initial benchmark model.\n",
    "Ideal for messy data (e.g., missing values, outliers, skewed data) due to its robustness.\n",
    "- **When Not to Use Bagging**:\n",
    "Interpretability: Difficult to understand the combined effect across hundreds of decision trees.\n",
    "Not Always Optimal: Might not capture the full signal in the data, so if 100% accuracy is required, consider other more powerful algorithms.\n",
    "Slower Prediction Time: While parallelizable in training, bagging models can be slower in making predictions due to the deeper trees.\n",
    "\n",
    "- **Random Forest**: Random Forest is an ensemble method that builds multiple independent decision trees using the bagging technique, enhancing prediction accuracy and stability.\n",
    "  \n",
    "    - Parallelization: Independent trees allow for parallel training, making Random Forest faster than boosting methods, where trees are dependent.\n",
    "    - Reduced Correlation: Ensures trees represent unique sections of data, avoiding redundancy and reducing model variance.\n",
    "    - Applicability:\n",
    "        * Useful for both regression and classification problems.\n",
    "        * For regression: Averages outputs from each tree.\n",
    "        * For classification: Uses voting across all trees for the final output.\n",
    "    - Bagging with a Twist:\n",
    "        * Standard Bagging: Uses training data samples with replacement to build each tree.\n",
    "        * Random Forest Wrinkle: Samples both data examples and features for each tree, ensuring each tree learns different patterns in subsets of data and features.\n",
    "        * Results in high variance models with uncorrelated trees.\n",
    "    - Benefits:\n",
    "        * Flexibility & Robustness: Performs well on messy data with mixed data types.\n",
    "        * Efficiency: Trains quickly with low risk of overfitting, making it a widely-used algorithm for machine learning practitioners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d12519-8463-4bc5-865c-7d13cc82a486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80e20f-e6bb-407b-a0b0-76d40940d2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
